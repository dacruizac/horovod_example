{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkConf, Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Embedding, Concatenate, Dense, Flatten, Reshape, BatchNormalization, Dropout\n",
    "\n",
    "import horovod.spark.keras as hvd\n",
    "from horovod.spark.common.backend import SparkBackend\n",
    "from horovod.spark.common.store import Store\n",
    "from pyspark.ml.feature import Normalizer, MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import csv\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "args = {}\n",
    "args['master'] = \"local[{}]\"\n",
    "args['learning_rate'] = 0.001\n",
    "args['batch-size'] = 100\n",
    "args['epochs'] = 10\n",
    "args[\"num-proc\"] = 4\n",
    "args['data-file'] = \"/opt/train_data/auto-mpg.data\"\n",
    "\n",
    "# Create Spark session for data preparation.\n",
    "conf = SparkConf().setAppName('Keras linear model example').set('spark.sql.shuffle.partitions', '16')\n",
    "conf.setMaster(args[\"master\"].format(args[\"num-proc\"]))\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('================')\n",
    "print('Data preparation')\n",
    "print('================')\n",
    "\n",
    "def normalize_pytorch_dataset(dataset, \n",
    "                              output_columns : list = [], \n",
    "                              model = None):\n",
    "    feature_columns = dataset.columns\n",
    "    # print(feature_columns)\n",
    "    if model == None:\n",
    "        assembler = VectorAssembler(inputCols=feature_columns,outputCol=\"Vectors\")\n",
    "        pipeline = Pipeline(stages=[assembler])\n",
    "        df = pipeline.fit(dataset).transform(dataset)\n",
    "        standardScaler = StandardScaler()\n",
    "        standardScaler.setInputCol(\"Vectors\")\n",
    "        standardScaler.setOutputCol(\"scaled\")\n",
    "        model_scaler = standardScaler.fit(df)\n",
    "    else:\n",
    "        model_scaler = model\n",
    "    normalized_dataset = dataset.withColumn(feature_columns[0],col=dataset[feature_columns[0]])\n",
    "    for i in range(len(feature_columns)):\n",
    "        column_name = feature_columns[i]\n",
    "        if column_name not in output_columns:\n",
    "            normalized_dataset = normalized_dataset.withColumn(column_name, \n",
    "                                            (dataset[column_name] - model_scaler.mean[i])/ \\\n",
    "                                            model_scaler.std[i] )\n",
    "    return normalized_dataset, model_scaler\n",
    "\n",
    "target = spark.sparkContext.textFile(args['data-file'])\n",
    "\n",
    "df = target.map(lambda line: line.split(\"\\t\")[0]).filter(lambda line: '?' not in line)\n",
    "df = df.mapPartitions(lambda partition: csv.reader([line.replace('\\0','') for line in partition], delimiter=' ',\n",
    "                                                    skipinitialspace=True, quotechar='\"',))\n",
    "\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "cust_schema = [StructField(col, StringType(), True) for col in column_names]\n",
    "cust_schema = StructType(cust_schema)\n",
    "\n",
    "df = spark.createDataFrame(data=df, schema = cust_schema)\n",
    "# df = df.select(*[col(c).cast(\"float\") for c in df.columns])\n",
    "\n",
    "# df.show(1)\n",
    "# print(df.select(\"MPG\").collect()[0])\n",
    "df = df.withColumn(\"USA\",df[\"origin\"]==\"1\")\n",
    "df = df.withColumn(\"Europe\",df[\"origin\"]==\"2\")\n",
    "df = df.withColumn(\"Japan\",df[\"origin\"]==\"3\")\n",
    "df = df.drop(\"Origin\")\n",
    "df = df.select(*[col(c).cast(\"float\") for c in df.columns])\n",
    "dataset = df\n",
    "# df.show(1)\n",
    "\n",
    "for i in range(len(dataset.columns)):\n",
    "    column_name = dataset.columns[i]\n",
    "    new_name = column_name.replace(' ','_')\n",
    "    if column_name != new_name:\n",
    "        dataset = dataset.withColumnRenamed(column_name, new=new_name)\n",
    "\n",
    "print(dataset.columns)\n",
    "\n",
    "splits_data = dataset.randomSplit([0.8, 0.2],0)\n",
    "train_dataset = splits_data[0]\n",
    "test_dataset = splits_data[1]\n",
    "\n",
    "n_train, model_train = normalize_pytorch_dataset(train_dataset, output_columns=[\"MPG\"])\n",
    "n_test, _ = normalize_pytorch_dataset(test_dataset, output_columns=[\"MPG\"], model=model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============================')\n",
    "print('Tensorflow model construction')\n",
    "print('=============================')\n",
    "\n",
    "def build_model(columns):\n",
    "  inputs = [Input(shape=(1,), name=col) for col in columns]\n",
    "  x = Concatenate()(inputs)\n",
    "  x = Dense(16, activation='relu')(x)\n",
    "  x = Dense(16, activation='relu')(x)\n",
    "  output = Dense(1)(x)\n",
    "  model = tf.keras.Model(inputs, output)\n",
    "\n",
    "  # model = keras.Sequential([\n",
    "  #   Dense(64, activation='relu', input_dim=input_s),\n",
    "  #   Dense(64, activation='relu'),\n",
    "  #   Dense(1)\n",
    "  # ])\n",
    "\n",
    "  # optimizer = tf.keras.optimizers.RMSprop(args['learning_rate'])\n",
    "\n",
    "  # model.compile(loss='mse',\n",
    "  #               optimizer=optimizer,\n",
    "  #               metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "\n",
    "features = [i for i in n_train.columns if i not in [\"MPG\"]]\n",
    "# input_shape = len(n_train.columns) - 1\n",
    "# print(input_shape)\n",
    "model = build_model(features)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('======================')\n",
    "print('Hovorod model training')\n",
    "print('======================')\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(args['learning_rate'])\n",
    "loss='mse'\n",
    "metrics=['mae', 'mse']\n",
    "store = Store.create('/home/daniel/Documentos/AWS/apache_spark/datasets/')\n",
    "\n",
    "features = [i for i in n_train.columns if i not in [\"MPG\"]]\n",
    "print(features)\n",
    "\n",
    "keras_estimator = hvd.KerasEstimator(\n",
    "#    num_proc=1,\n",
    "    store=store,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    feature_cols=features,\n",
    "    label_cols=['MPG'],\n",
    "    batch_size=args['batch-size'],\n",
    "    epochs=args['epochs'],\n",
    "    )\n",
    "\n",
    "keras_model = keras_estimator.fit(n_train).setOutputCols(['MPG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------\")\n",
    "print(\"Final prediction\")\n",
    "print(\"----------------\")\n",
    "\n",
    "n_test = n_test.drop(\"MPG\")\n",
    "\n",
    "pred_df=keras_model.transform(n_test)\n",
    "pred_df.printSchema()\n",
    "pred_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
